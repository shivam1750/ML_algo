{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\kaifk\\lpth\\.vscode\\DataSciencePrac\\003-Support-Vector-Machine-SVM-Example\\applicant.csv\")\n",
    "df\n",
    "names=[\n",
    "        \"Age\",\"Workclass\",\"fnlwgt\",\"Education\",\"Education-Num\",\"Marital Status\",\n",
    "        \"Occupation\",\"Relationship\",\"Race\",\"Gender\",\"Capital Gain\",\"Capital Loss\",\n",
    "        \"Hours per week\",\"Country\",\"Target\"]\n",
    "df.columns = names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum()) #duplicated value encountered \n",
    "# percentage of duplicated values \n",
    "print(np.round(100*df.duplicated().sum()/len(df),1),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values \n",
    "df.isna().sum() \n",
    "# no null values found \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check for the propotion of target variable ok i.e Target distribution\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "df.Target.value_counts().plot.pie(explode=[0.1,0.1],autopct = '%1.1f%%').set_title(\"Target Distribution \")\n",
    "# can be seen that this is a  dispropotionate target distribution and to deal with this we might need to over sample the minority i.e >50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets analyze the features and plot them to better understand them with respect to the target \n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(data = df , x = 'Age',hue='Target',kde =True,binwidth=1)\n",
    "plt.title(\"Age count with respect to Target \")\n",
    "plt.show()\n",
    "# age is not playing a major role here but for the consideration we can take it as \n",
    "# age ranging from 20 - 80 are more likely to be in >50k category \n",
    "# and for the other it is not linearly dependent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectintcol = df.select_dtypes('int').columns\n",
    "selectintcol\n",
    "fig=plt.figure(figsize=(20,30))\n",
    "for i, var_name in enumerate(selectintcol):\n",
    "    # Left plot\n",
    "    ax=fig.add_subplot(6,2,2*i+1)\n",
    "    sns.histplot(data=df, x=var_name, axes=ax, bins=30, kde=False, hue='Target')\n",
    "    ax.set_title(var_name)\n",
    "    \n",
    "    # Right plot (truncated)\n",
    "    ax=fig.add_subplot(6,2,2*i+2)\n",
    "    sns.histplot(data=df, x=var_name, axes=ax, bins=30, kde=True, hue='Target')\n",
    "    ax.set_title(var_name)\n",
    "fig.tight_layout()  # Improves appearance a bit\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no impact of captial gain and loss as they are merely all zero accross the population insight : drop them \n",
    "# Education num is a good feature \n",
    "# other features are no good use so we are keeping education - num as only feature selected from the dtypes int \n",
    "# fnlwght is also linearly independent \n",
    "df['Capital Loss'].value_counts().plot.pie()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Capital Gain'].value_counts().plot.pie()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data viz \n",
    "cat_cols = df.select_dtypes('object').drop(['Target'], axis=1)\n",
    "cat_cols = cat_cols.columns\n",
    "\n",
    "# Plot categorical features\n",
    "fig = plt.figure(figsize=(10, 16))\n",
    "for i, var_name in enumerate(cat_cols):\n",
    "    ax = fig.add_subplot(8, 1, i + 1)\n",
    "    sns.countplot(data=df, x=var_name, ax=ax, hue='Target')\n",
    "    ax.set_title(var_name)\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "we have found great results from this \n",
    "     - firstly we can clearly observe that the private sector workclass has the most > 50k and as well as the <=50k  \n",
    "     - majority of >50k has education background as Bachleors  majority of <=50k have education as ms-grad \n",
    "     -  Married-civ-spouse has the highest >50k never married has the highest <=50k \n",
    "     - occupation plays a major role in identifying the target value  \n",
    "     - relationship : husband is more likely to earn >50k and not in family person is <=50k\n",
    "     - race (white dominant)\n",
    "     - gender (male dominant in both aspect ) (crucial feature as most of females are <=50k)\n",
    "     - country (restricted to US only as population majority lies there )\n",
    "```\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat colms to encode : \n",
    "# important colms are gender ,education num , occupation , relationship , working hours\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encodingcolms = ['Gender','Occupation','Relationship','Target']\n",
    "def encode_colms(df,colms):\n",
    "    le = LabelEncoder()\n",
    "    for col in colms:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "encode_colms(df,encodingcolms)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Education-Num')['Target'].mean()\n",
    "df.groupby('Education-Num').Target.mean().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we had seen that the male samples were over the females one so this might make model biased \n",
    "df['Gender'].value_counts().plot.pie(explode=[0.1, 0.1], autopct='%1.1f%%', labels=['Male', 'Female']).set_title(\"Gender Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "x = df[['Gender','Occupation','Relationship','Education-Num','Age']]\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y,train_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform cross-validation with StratifiedKFold and get predictions\n",
    "Svm  = SVC(kernel='linear', random_state=40)\n",
    "Svm.fit(X_train,y_train)\n",
    "y_pred_cv= Svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_cv)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_cv)\n",
    "classification_rep = classification_report(y_test, y_pred_cv)\n",
    "print(f\"accuracy : {accuracy} \\n conffusion matrix :\\n {conf_matrix} \\n classifaction report \\n:{classification_rep}\\n\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a Nueral Net and XGB to test if the accuracy varies along the models \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def xgb(X_train,X_test,y_train,y_test):\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [150, 180, 220],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    xgb_classifier = XGBClassifier(random_state=42)\n",
    "    # Create GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Make predictions on the test data using the best model\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Best Hyperparameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
    "# Test Accuracy: 0.8259\n",
    "xgb(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential nueral net \n",
    "\n",
    "def nueralnet(X_train,X_test,y_train,y_test):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(5,)),  \n",
    "            tf.keras.layers.Dense(64, activation='tanh'),  # Dense hidden layer with ReLU activation\n",
    "            tf.keras.layers.Dense(32,activation='tanh'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "nueralnet(X_train,X_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Accuracy is as follows : \n",
    "    </br>\n",
    "    - nueral net with 2 hidden layer using activation function as tanh gave the highest accuracy that is `**82.67**`\n",
    "    <br/>\n",
    "    - xgb classifier with best param as Best Hyperparameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 150, 'subsample': 0.9} gave an accuracy of 82.59 %\n",
    "    <br/>\n",
    "    - svm gave the lowest accuracy 80.7 % \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over sampling failed\n",
    "# from sklearn.utils import shuffle\n",
    "# #  Split the data into male and female groups\n",
    "# X_train_male = X_train[df['Gender'] == 1]\n",
    "# y_train_male = y_train[df['Gender'] == 1]\n",
    "\n",
    "# X_train_female = X_train[df['Gender'] == 0]\n",
    "# y_train_female = y_train[df['Gender'] == 0]\n",
    "\n",
    "# # Oversample the minority class only for females\n",
    "# oversample_factor = 2 # You can adjust this factor based on your preference\n",
    "\n",
    "# # Duplicate the original samples for females\n",
    "# X_resampled_female = X_train_female\n",
    "# y_resampled_female = y_train_female\n",
    "\n",
    "# # Apply SMOTE to create synthetic samples for females\n",
    "# smote_female = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "# X_synthetic_female, y_synthetic_female = smote_female.fit_resample(X_train_female, y_train_female)\n",
    "\n",
    "# # Combine the original and synthetic samples\n",
    "# X_resampled_female = pd.concat([X_resampled_female] + [X_synthetic_female] * (oversample_factor - 1))\n",
    "# y_resampled_female = pd.concat([y_resampled_female] + [y_synthetic_female] * (oversample_factor - 1))\n",
    "# # Combine the resampled data for both genders\n",
    "# X_resampled = pd.concat([X_train_male, X_resampled_female])\n",
    "# y_resampled = pd.concat([y_train_male, y_resampled_female])\n",
    "\n",
    "# X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=42)\n",
    "# X_resampled.Gender.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
